{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRtXgwNtw5KWdSSyUgoz3C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dikchik9100/genome/blob/main/Untitled8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS_WcYSKpjfb",
        "outputId": "b0a73749-6f45-4fdf-f35a-840c4a0a0bd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting openml\n",
            "  Downloading openml-0.15.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting liac-arff>=2.4.0 (from openml)\n",
            "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting xmltodict (from openml)\n",
            "  Downloading xmltodict-1.0.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from openml) (2.32.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from openml) (2.9.0.post0)\n",
            "Collecting minio (from openml)\n",
            "  Downloading minio-7.2.18-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from openml) (18.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openml) (4.67.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from openml) (25.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->openml) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from minio->openml) (25.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from minio->openml) (2025.8.3)\n",
            "Collecting pycryptodome (from minio->openml)\n",
            "  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from minio->openml) (4.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (from minio->openml) (2.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->openml) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->openml) (3.10)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->minio->openml) (25.1.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi->minio->openml) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio->openml) (2.23)\n",
            "Downloading openml-0.15.1-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading minio-7.2.18-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xmltodict-1.0.2-py3-none-any.whl (13 kB)\n",
            "Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: liac-arff\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11717 sha256=04148b1488fb7171c98972296e7172e31ec8d4d1c289f8dbf0c573c269291716\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/ac/cf/c2919807a5c623926d217c0a18eb5b457e5c19d242c3b5963a\n",
            "Successfully built liac-arff\n",
            "Installing collected packages: xmltodict, pycryptodome, liac-arff, minio, openml\n",
            "Successfully installed liac-arff-2.5.0 minio-7.2.18 openml-0.15.1 pycryptodome-3.23.0 xmltodict-1.0.2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install scikit-learn openml pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import openml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "def fetch_leukemia_dataset():\n",
        "    # Find a leukemia gene-expression dataset (binary ALL vs AML)\n",
        "    datasets = openml.datasets.list_datasets(output_format=\"dataframe\")\n",
        "    candidates = datasets[\n",
        "        datasets['name'].str.contains('leukemia', case=False, na=False)\n",
        "        & (datasets['NumberOfClasses'] == 2)\n",
        "        & (datasets['NumberOfInstances'] >= 60)\n",
        "        & (datasets['NumberOfFeatures'] > 1000)  # gene-expression typically thousands of probes\n",
        "    ].sort_values(by='NumberOfInstances', ascending=False)\n",
        "\n",
        "    if candidates.empty:\n",
        "        raise RuntimeError(\"Could not find a suitable leukemia dataset on OpenML.\")\n",
        "\n",
        "    did = int(candidates.iloc[0]['did'])\n",
        "    ds = openml.datasets.get_dataset(did)\n",
        "    X, y, categorical_indicator, attribute_names = ds.get_data(target=ds.default_target_attribute)\n",
        "\n",
        "    X = pd.DataFrame(X, columns=attribute_names[:-1] if len(attribute_names)==X.shape[1]+1 else attribute_names)\n",
        "    y = pd.Series(y, name='target')\n",
        "    return X, y, ds\n",
        "\n",
        "def train_and_evaluate(X, y):\n",
        "    # Many gene-expression datasets are high-dimensional and low-sample\n",
        "    # A simple, strong baseline: StandardScaler + L2 Logistic Regression\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
        "        ('clf', LogisticRegression(\n",
        "            penalty='l2',\n",
        "            solver='liblinear',\n",
        "            max_iter=5000,\n",
        "            class_weight='balanced',  # handle any imbalance\n",
        "            random_state=42\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    # Stratified CV for robust estimate\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
        "    print(f\"CV accuracy: mean={cv_scores.mean():.3f}, std={cv_scores.std():.3f}\")\n",
        "\n",
        "    # Fit final model on train split for a report and prediction demo\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.25, stratify=y, random_state=42\n",
        "    )\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    print(\"\\nHoldout accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "    return pipeline, X_train.columns.tolist(), np.unique(y)\n",
        "\n",
        "def predict_for_person(model, feature_names, class_labels, person_features_dict):\n",
        "    # person_features_dict: dict mapping feature_name -> value\n",
        "    # Any missing features will be filled with 0; ensure names align\n",
        "    x_row = np.zeros((1, len(feature_names)), dtype=float)\n",
        "    name_to_idx = {n: i for i, n in enumerate(feature_names)}\n",
        "    for k, v in person_features_dict.items():\n",
        "        if k in name_to_idx:\n",
        "            x_row[0, name_to_idx[k]] = float(v)\n",
        "\n",
        "    pred = model.predict(x_row)[0]\n",
        "    proba = None\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        proba_arr = model.predict_proba(x_row)[0]\n",
        "        proba = {str(class_labels[i]): float(proba_arr[i]) for i in range(len(class_labels))}\n",
        "\n",
        "    return pred, proba\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Downloading leukemia dataset from OpenML...\")\n",
        "    X, y, ds_meta = fetch_leukemia_dataset()\n",
        "    print(f\"Dataset: {ds_meta.name} (did={ds_meta.dataset_id}), samples={X.shape[0]}, features={X.shape[1]}\")\n",
        "\n",
        "    model, feature_names, class_labels = train_and_evaluate(X, y)\n",
        "\n",
        "    # Example: predict for a (synthetic) person by setting a few probe values.\n",
        "    # In practice, you'd map this person's measured gene-expression values to these exact feature names.\n",
        "    example_input = {\n",
        "        # \"gene_or_probe_name\": value,\n",
        "        # Only a few shown; absent ones default to 0 in this demo\n",
        "        feature_names[0]: 0.5,\n",
        "        feature_names[1]: -1.2,\n",
        "        feature_names[2]: 2.3\n",
        "    }\n",
        "    pred_label, proba = predict_for_person(model, feature_names, class_labels, example_input)\n",
        "    print(\"\\nPredicted disease class:\", pred_label)\n",
        "    if proba is not None:\n",
        "        print(\"Class probabilities:\", proba)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fteSJXkbqBYN",
        "outputId": "5bd10d99-9443-4f4b-d262-86e07d064d14"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading leukemia dataset from OpenML...\n",
            "Dataset: leukemia (did=1104), samples=72, features=7129\n",
            "CV accuracy: mean=0.945, std=0.050\n",
            "\n",
            "Holdout accuracy: 0.8333333333333334\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         ALL       0.91      0.83      0.87        12\n",
            "         AML       0.71      0.83      0.77         6\n",
            "\n",
            "    accuracy                           0.83        18\n",
            "   macro avg       0.81      0.83      0.82        18\n",
            "weighted avg       0.84      0.83      0.84        18\n",
            "\n",
            "\n",
            "Predicted disease class: ALL\n",
            "Class probabilities: {'ALL': 0.9999906630663873, 'AML': 9.33693361266593e-06}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}